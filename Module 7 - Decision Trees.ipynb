{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 - Decision Trees\n",
    "\n",
    "Decision tree is a classification algorithm (though there are regression trees) for supervised learning, and is popular for its ease-of-use and generally good performance. It learns different decision boundaries by segmenting the dataset into groups based on similar attributes and target outcomes. This technique is called **recursive partitioning**.\n",
    "\n",
    "When new data is given to the decision tree model for prediction, it starts from the **root node** (most important/significant decision for prediction), then follows the path of **decision nodes** that ultimately leads it to a **terminal node** with the predicted category.\n",
    "\n",
    "![Decision Tree](https://miro.medium.com/max/2649/1*iMOtF7bwKPHl1Pg52xN7fg.png)\n",
    "Source: [Towards Data Science: Decision Tree in Layman's Terms](https://towardsdatascience.com/decision-tree-in-laymans-terms-part-1-76e1f1a6b672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Survival\n",
    "\n",
    "In this lesson, we will clean and prepare the dataset of Titanic passengers using the same methods as the previous lesson, in order to build a decision tree model. Then we will compare the results of both the logistic regression and decision tree models to determine which one is the better performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import functions directly from sci-kit learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "filepath = \"datasets/titanic.xls\"\n",
    "\n",
    "df = pd.read_excel(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "The dataset contains the following features (characteristics) in the columns:\n",
    "\n",
    "- `pclass`: passenger class (1 = 1st class, 2 = 2nd class, 3 = 3rd class) \n",
    "- `survived`: survival status (0 = No(died), 1 = Yes(survived))\n",
    "- `name`: passenger's name\n",
    "- `sex`: passenger's sex (male/female)\n",
    "- `age`: passenger's age\n",
    "- `sibsp`: number of siblings and/or spouses with passenger\n",
    "- `parch`: number of parents and/or children with passenger\n",
    "- `ticket`: ticket number\n",
    "- `fare`: total fare for passenger and others in party (currency: British Pound)\n",
    "- `cabin`: room cabin number(s) for passenger and their party\n",
    "- `embarked`: port of embarkation (C = Chernbourg, Q = Queenstown, S = Southampton)\n",
    "- `boat`: lifeboat name (combination of letters and/or numbers)\n",
    "- `body`: body identification number\n",
    "- `home.dest`: hometown or destination after disembark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Prepare Data\n",
    "\n",
    "After conducting exploratory analysis, we need to clean up the data and prepare it in a format for the predictive model. Columns relevant for our model will be cleaned and prepared, while colums and rows that are not significant for prediction will be removed. \n",
    "\n",
    "`age` and `embarked` are the only columns with missing values that will be used in the predictive model. We will fill in the missing information with a \"guesstimate\" and all the other columns with missing values will be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns with missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `age` column\n",
    "\n",
    "Because there are many values missing in the `age` column, we will create an estimate value for `age` that's specific to a person's survival status, as well as other significant characteristics like sex and passenger class (a proxy for socio-economic status).\n",
    "\n",
    "The `.transform()` function creates a single column that for every row of data, when the row matches the characteristics in a row of the `groupby`, it will take on that value. In this example, the `.transform()` produces a column where each row (passenger) has a mean average age, based on the passenger's survival status, sex, and passenger class.\n",
    "\n",
    "Then we will take the transformed column and using the `.fillna()` function, take the value from the transformed row and only use it in the corresponding row in the dataframe if the passenger's age is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average age grouped by survival status, sex, and passenger class\n",
    "df.groupby(['survived', 'sex', 'pclass'])['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store transform column as a variable\n",
    "tranform_age = df.groupby(['survived', 'sex', 'pclass'])['age'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values for age using values from transformed column\n",
    "df['age'].fillna(tranform_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify there are no more missing age values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `embarked` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the values in the `embarked` column are string categories, we can't use statistical methods to impute the missing information. There are very few rows of missing data, so we can fill in those values with the most common port of embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of passengers for each embarkation port\n",
    "df['embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with \"S\" for Southampton (most common port)\n",
    "df['embarked'].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify no missing values in 'embarked' column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary columns\n",
    "\n",
    "Now that the values are filled for columns that will be used in the predictive model, we can remove the columns that we do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that will not be used in the model\n",
    "modeldf = df.drop(['name','ticket','fare', 'cabin', 'boat', 'body', 'home.dest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in the new dataframe\n",
    "modeldf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Some columns in the data are object types that have string values that cannot be used in the algorithm function. During the Module 4 lesson, we transformed ordinal qualitative values into a numerical representation that preserved their ranking. For nominal (non-ordered) data, there is no ranking, so we will use **one-hot encoding** to numerically represent the values.\n",
    "\n",
    "One-hot encoding is a technique that takes discrete (categorical) data values from a column and creates a new column for each distinct category value. Within each column, the values `0` or `1` will be assigned, indicating a `True (1)` or `False (0)` value for that category. The one-hot encoded columns created are called **dummy variables**. \n",
    "\n",
    "The `pd.get_dummies()` function extracts the categories from a column, then makes them into dummy variables and assigns the boolean values. `pd.get_dummies` will automatically drop the column that was used as the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables for embarkation port\n",
    "modeldf = pd.get_dummies(data=modeldf, columns=['embarked'])\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical sex values as boolean\n",
    "\n",
    "Boolean values are very common to use to represent binary (two options) categorical data. For the model, we will reassign the string values for sex as boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign 'female'= 0, 'male'= 1\n",
    "modeldf['sex'] = modeldf['sex'].map({'female':0, 'male':1})\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine family member total\n",
    "\n",
    "In the dataset, there are separate columns for immediate family members that are in the same generation as the passenger (`sibsp` - sibling/spouse) or different generations (`parch` - parent/child). During the incident of the Titanic sinking, if a passenger were traveling with any family members then we should account for them all in a single column. Furthermore, we can also hypothesize that the more family members a passenger is traveling with, the more difficult it would have been to quickly move everybody to safety. For this reason, more family members might be linked to decreased survival likelihood, which is why we will create a new `family_num` column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column based on number of family members\n",
    "modeldf['family_num'] = modeldf['sibsp'] + modeldf['parch']\n",
    "\n",
    "# drop sibsp and parch columns\n",
    "modeldf.drop(['sibsp', 'parch'], axis=1, inplace=True)\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling\n",
    "\n",
    "The data is \"done\" being cleaned and prepared, so now we can build, or **fit**, our decision tree model. There are a few final tasks that need to be done before the data is given to the model:\n",
    "\n",
    "- Separate the attributes (features used to predict) from the target (outcome to predict)\n",
    "- Shuffle the order of the rows in the dataset, then separate into a dataset for training (for the model to learn from) and testing (to see how well it predicts with new data)\n",
    "\n",
    "When the model finishes \"learning\" with the training data, we will evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate attributes and target\n",
    "\n",
    "The target is the column of data we are teaching the model to predict. In math, this information is typically represented as the variable `y`, so we keep the same conventions. Attributes (characteristics) that calculate/predict `y` are stored into a variable called `X`. Although `y` is a single column, `X` is a dataframe of all the attribute columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'survived' is target variable\n",
    "y = modeldf['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes are all the columns EXCEPT 'survived'\n",
    "X = modeldf.drop(['survived'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate training and test data\n",
    "\n",
    "Scikit-learn's `train_test_split()` function takes the attribute columns (`X` variable) and target column (`y` variable), then shuffles the rows using the `random_state=` argument, which will trigger a randomizing sequence (see the [`random_state` documentation](https://scikit-learn.org/stable/glossary.html#term-random-state) for more information). By default, `test_size=` will separate 25% of the dataset as the test set, leaving the other 75% for the training set. However, you can adjust the value of the ratio split.\n",
    "\n",
    "`train_test_split()` then generates four outputs in this order - a dataframe of the attributes for the training set (`X_train`), a dataframe of the attributes for the test set (`X_test`), a column of the target for the training set (`y_train`), and a column of the target for the test set (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate 80% for training data, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "The `DecisionTreeClassifier()` function will take the `X_train` and `y_train` dataset, and calculate the attributes' segmented groups and probabilities that best fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign decision tree function to variable\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give training data to learn\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall ratio of correct predictions for training data\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test data\n",
    "\n",
    "To assess how well the model will perform on new data, we will use the test set to:\n",
    "\n",
    "- Display the ratio of overall correct predictions\n",
    "- Compare the number of correct and incorrect predictions for each target category\n",
    "- Compare the ratio of correct predictions for all actual target values and all predicted values for a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall ratio of correct predictions for test data\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare how many items in each category model predicted correctly vs incorrectly\n",
    "\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    columns=['Predicted: Died', 'Predicted: Survived'],\n",
    "    index=['Actual: Died', 'Actual: Survived']\n",
    ")\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ratio of correct predictions vs all predicted values for each category (precision)\n",
    "# compare ratio of correct predictions vs all actual values for each category (recall)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
