{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 - Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorithm for supervised learning, and is a smoothed-out version of linear regression. However, instead of a linear trend line extending infinitely in both directions, the y-axis values (the target outcome) of logistic regression trend line (a **sigmoid** \"S\" shape) can only take on values between 0 and 1. These values represent the likelihood/probability of being in the target outcome value assigned to `1`.\n",
    "\n",
    "![Logistic Regression](https://notebooks.azure.com/priesterkc/projects/images/raw/logistic_regression.png)\n",
    "Source: [javaTpoint: Logistic Regression in Machine Learning](https://www.javatpoint.com/logistic-regression-in-machine-learning)\n",
    "\n",
    "Logistic regression is typically used to classify a target with binary (two categories/options) outcomes, such as `Yes/No`, `True/False`, `spam/not spam`. The \"most true\" category will be numerically-encoded as `1` with the other category labeled `0`. The logistic regression function will also have a threshold value (probability cutoff) to predict a data point as one of the target categories, based on the x-axis value(s) (features) of the data point. On occasion, logistic regression can also be used for classifying when there are 3 or more target category outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Survival\n",
    "\n",
    "In this lesson, we will clean and prepare the dataset of Titanic passengers in order to build a logistic regression model. The model will predict a passenger's survival status (`0`= died, `1`= survived) based on certain characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import functions directly from sci-kit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "filepath = \"datasets/titanic.xls\"\n",
    "\n",
    "df = pd.read_excel(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "The dataset contains the following features (characteristics) in the columns:\n",
    "\n",
    "- `pclass`: passenger class (1 = 1st class, 2 = 2nd class, 3 = 3rd class) \n",
    "- `survived`: survival status (0 = No(died), 1 = Yes(survived))\n",
    "- `name`: passenger's name\n",
    "- `sex`: passenger's sex (male/female)\n",
    "- `age`: passenger's age\n",
    "- `sibsp`: number of siblings and/or spouses with passenger\n",
    "- `parch`: number of parents and/or children with passenger\n",
    "- `ticket`: ticket number\n",
    "- `fare`: total fare for passenger and others in party (currency: British Pound)\n",
    "- `cabin`: room cabin number(s) for passenger and their party\n",
    "- `embarked`: port of embarkation (C = Chernbourg, Q = Queenstown, S = Southampton)\n",
    "- `boat`: lifeboat name (combination of letters and/or numbers)\n",
    "- `body`: body identification number\n",
    "- `home.dest`: hometown or destination after disembark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "We will do a simple exploratory data analysis to understand what information is in the dataset, and identify any relationships or patterns in the data. The exploratory tasks we will do are:\n",
    "\n",
    "- Compare the number of passengers by survival status\n",
    "- Compare the ratio of men vs women that survived\n",
    "- Compare the ratio of each passenger class that survived\n",
    "- Compare the distribution of passengers based on age and survival status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of passengers per survival status\n",
    "sns.countplot(data=df, x='survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of passengers by gender that survived\n",
    "sns.barplot(data=df, x='sex', y='survived', ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of passengers by class that survived\n",
    "sns.barplot(data=df, x='pclass', y='survived', ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.kdeplot()` function creates a kernel density estimate plot, which shows the distribution curve of the data. The y-axis refers to a probability of a value occuring in a particular area of the density plot, but we will not use this concept for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the age column with all rows of passengers that survived\n",
    "survive_age = df['age'].loc[df['survived'] == 1]\n",
    "\n",
    "# select the age column with all rows of passengers that died\n",
    "died_age = df['age'].loc[df['survived'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot to compare distribution of age by survival status\n",
    "\n",
    "# distribution of ages for passengers that survived\n",
    "sns.kdeplot(survive_age, color=\"darkturquoise\", shade=True)\n",
    "\n",
    "# distribution of ages for passengers that died\n",
    "sns.kdeplot(died_age, color=\"lightcoral\", shade=True)\n",
    "\n",
    "# add legend, chart title, and x-axis label\n",
    "plt.legend (['Survived', 'Died'])\n",
    "plt.title(\"Density Plot of Age for Survived vs Deceased Population\")\n",
    "plt.xlabel('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Prepare Data\n",
    "\n",
    "After conducting exploratory analysis, we need to clean up the data and prepare it in a format for the predictive model. Columns relevant for our model will be cleaned and prepared, while colums and rows that are not significant for prediction will be removed. \n",
    "\n",
    "`age` and `embarked` are the only columns with missing values that will be used in the predictive model. We will fill in the missing information with a \"guesstimate\" and all the other columns with missing values will be removed from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns with missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `age` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify rows where age is missing\n",
    "missing_age = df.loc[df['age'].isnull()]\n",
    "missing_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index positions of missing rows - we'll use this later\n",
    "mals = list(missing_age.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the earlier density plot, there were variations in the ratio of passengers for each survival status category, based on their age. Because there are many values missing in the `age` column, we will create an estimate value for `age` that's specific to a person's survival status, as well as other significant characteristics like sex and passenger class (a proxy for socio-economic status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average age grouped by survival status, sex, and passenger class\n",
    "df.groupby(['survived', 'sex', 'pclass'])['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.transform()` function creates a single column that for every row of data, when the row matches the characteristics in a row of the `groupby`, it will take on that value. In this example, the `.transform()` produces a column where each row (passenger) has a mean average age, based on the passenger's survival status, sex, and passenger class.\n",
    "\n",
    "Then we will take the transformed column and using the `.fillna()` function, take the value from the transformed row and only use it in the corresponding row in the dataframe if the passenger's age is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a column of mean age for each row (passenger)\n",
    "df.groupby(['survived', 'sex', 'pclass'])['age'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store transform column as a variable\n",
    "tranform_age = df.groupby(['survived', 'sex', 'pclass'])['age'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values for age using values from transformed column\n",
    "df['age'].fillna(tranform_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify filled missing values \n",
    "df.iloc[mals].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify there are no more missing age values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `embarked` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values for 'embarked'\n",
    "embark = df.loc[df['embarked'].isnull()]\n",
    "embark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save index for missing values to verify later\n",
    "embarkls = list(embark.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the values in the `embarked` column are string categories, we can't use statistical methods to impute the missing information. There are very few rows of missing data, so we can fill in those values with the most common port of embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of passengers for each embarkation port\n",
    "df['embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with \"S\" for Southampton (most common port)\n",
    "df['embarked'].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that values are filled\n",
    "df.iloc[embarkls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify no missing values in 'embarked' column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary columns\n",
    "\n",
    "Now that the values are filled for columns that will be used in the predictive model, we can remove the columns that we do not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that will not be used in the model\n",
    "modeldf = df.drop(['name','ticket','fare', 'cabin', 'boat', 'body', 'home.dest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in the new dataframe\n",
    "modeldf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Some columns in the data are object types that have string values that cannot be used in the algorithm function. During the Module 4 lesson, we transformed ordinal qualitative values into a numerical representation that preserved their ranking. For nominal (non-ordered) data, there is no ranking, so we will use **one-hot encoding** to numerically represent the values.\n",
    "\n",
    "One-hot encoding is a technique that takes discrete (categorical) data values from a column and creates a new column for each distinct category value. Within each column, the values `0` or `1` will be assigned, indicating a `True (1)` or `False (0)` value for that category. The one-hot encoded columns created are called **dummy variables**. \n",
    "\n",
    "The `pd.get_dummies()` function extracts the categories from a column, then makes them into dummy variables and assigns the boolean values. `pd.get_dummies` will automatically drop the column that was used as the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables for embarkation port\n",
    "modeldf = pd.get_dummies(data=modeldf, columns=['embarked'])\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical sex values as boolean\n",
    "\n",
    "Boolean values are very common to use to represent binary (two options) categorical data. For the model, we will reassign the string values for sex as boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign 'female'= 0, 'male'= 1\n",
    "modeldf['sex'] = modeldf['sex'].map({'female':0, 'male':1})\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine family member total\n",
    "\n",
    "In the dataset, there are separate columns for immediate family members that are in the same generation as the passenger (`sibsp` - sibling/spouse) or different generations (`parch` - parent/child). During the incident of the Titanic sinking, if a passenger were traveling with any family members then we should account for them all in a single column. Furthermore, we can also hypothesize that the more family members a passenger is traveling with, the more difficult it would have been to quickly move everybody to safety. For this reason, more family members might be linked to decreased survival likelihood, which is why we will create a new `family_num` column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column based on number of family members\n",
    "modeldf['family_num'] = modeldf['sibsp'] + modeldf['parch']\n",
    "\n",
    "# drop sibsp and parch columns\n",
    "modeldf.drop(['sibsp', 'parch'], axis=1, inplace=True)\n",
    "modeldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling\n",
    "\n",
    "The data is \"done\" being cleaned and prepared, so now we can build, or **fit**, our logistic regression model. There are a few final tasks that need to be done before the data is given to the model:\n",
    "\n",
    "- Separate the attributes (features used to predict) from the target (outcome to predict)\n",
    "- Shuffle the order of the rows in the dataset, then separate into a dataset for training (for the model to learn from) and testing (to see how well it predicts with new data)\n",
    "\n",
    "When the model finishes \"learning\" with the training data, we will evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate attributes and target\n",
    "\n",
    "The target is the column of data we are teaching the model to predict. In math, this information is typically represented as the variable `y`, so we keep the same conventions. Attributes (characteristics) that calculate/predict `y` are stored into a variable called `X`. Although `y` is a single column, `X` is a dataframe of all the attribute columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'survived' is target variable\n",
    "y = modeldf['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes are all the columns EXCEPT 'survived'\n",
    "X = modeldf.drop(['survived'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate training and test data\n",
    "\n",
    "Scikit-learn's `train_test_split()` function takes the attribute columns (`X` variable) and target column (`y` variable), then shuffles the rows using the `random_state=` argument, which will trigger a randomizing sequence (see the [`random_state` documentation](https://scikit-learn.org/stable/glossary.html#term-random-state) for more information). By default, `test_size=` will separate 25% of the dataset as the test set, leaving the other 75% for the training set. However, you can adjust the value of the ratio split.\n",
    "\n",
    "`train_test_split()` then generates four outputs in this order - a dataframe of the attributes for the training set (`X_train`), a dataframe of the attributes for the test set (`X_test`), a column of the target for the training set (`y_train`), and a column of the target for the test set (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate 80% for training data, 20% for test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "The `LogisticRegression()` function will take the `X_train` and `y_train` dataset, and calculate the attributes' parameters (weights) and sigmoid line shape that best fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign logistic regression function to variable\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give training data to learn\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall ratio of correct predictions for training data\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test data\n",
    "\n",
    "To assess how well the model will perform on new data, we will use the test set to:\n",
    "\n",
    "- Display the ratio of overall correct predictions\n",
    "- Compare the number of correct and incorrect predictions for each target category\n",
    "- Compare the ratio of correct predictions for all actual target values and all predicted values for a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall ratio of correct predictions for test data\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare how many items in each category model predicted correctly vs incorrectly\n",
    "\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    columns=['Predicted: Died', 'Predicted: Survived'],\n",
    "    index=['Actual: Died', 'Actual: Survived']\n",
    ")\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ratio of correct predictions vs all predicted values for each category (precision)\n",
    "# compare ratio of correct predictions vs all actual values for each category (recall)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://nbviewer.jupyter.org/github/BigDataGal/Data-Mania-Demos/blob/master/Logistic%20Regression%20Demo.ipynb\n",
    "\n",
    "https://mashimo.wordpress.com/2018/03/31/logistic-regression-using-sklearn/\n",
    "\n",
    "https://www.kaggle.com/mnassrib/titanic-logistic-regression-with-python/notebook\n",
    "\n",
    "https://datascienceplus.com/would-you-survive-the-titanic-getting-started-in-python/\n",
    "\n",
    "https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
