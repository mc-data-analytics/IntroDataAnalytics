{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Data Processing\n",
    "After getting an understanding of the dataset in the EDA phase, the data must then be cleaned and transformed for one or more of these next steps - statistical analysis, reporting (with data visualizations), or predictive modeling. You may end up needing different transformations of the same dataset for all of these purposes throughout the data project life cycle.\n",
    "\n",
    "In this lesson, we will take the information gathered during the EDA to clean up the \"messy\" data (missing values, duplicates) and also restructure some columns of data into a more useable format, depending on the next step it will be used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "filepath = \"datasets/gradedata_sn.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace values with null\n",
    "\n",
    "Sometimes when data is being updated by a person/team that does not understand data storage best practices or the data was collected through surveys, missing or involuntary information is typicalled filled in as `NA`, `not applicable`, `unknown`, `-`, etc. When the data is then read into Python it interprets those filled values as strings, and if its mixed in a column that contains numeric data, `pandas` will assign the data type as an object.\n",
    "\n",
    "We can replace the string values with an acutal missing information (blank cell) using the `np.nan` function, which creates a null value. To update the string values, use the syntax `df.loc[df[column] == string_val, column] = new_value`. This will select the rows where the column's value matches the string value and then within the same or a different column, assign the new value to those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distinct categories in the 'gender' column\n",
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where the \"no answer\" values are\n",
    "df.loc[df['gender'] == 'no answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the value \"no answer\" in the column and replace it w/ a null value\n",
    "df.loc[df['gender'] == 'no answer', 'gender'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the rows where the 'gender' value is missing (null)\n",
    "# check if the values updated properly\n",
    "df.loc[df['gender'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there are now 2 missing values in the 'gender' column\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Column Data Type\n",
    "\n",
    "Through additional exploration, you will find that the `age`, `exercise`, `hours`, and `grade` columns have `'no answer'` string values, rather than empty spaces for missing information. These columns have numeric data and even if the string values were replaced with null values, the data type of the column would still remain an object.\n",
    "\n",
    "The `pd.to_numeric` will take in a column of data and convert the values to the numeric type (int or float) that it interprets. The argument `errors='coerce'` will convert any value that causes an error into a `NaN`. In this example, the strings values will cause an error because they cannot be converted to a number, and will be replaced with a null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object to Float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the values in 'exercise' column to numbers\n",
    "df['exercise'] = pd.to_numeric(df['exercise'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data type of 'exercise' column is now float\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are now missing values\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally convert 'hours' and 'grade' columns\n",
    "df['hours'] = pd.to_numeric(df['hours'], errors='coerce')\n",
    "df['grade'] = pd.to_numeric(df['grade'], errors='coerce')\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another check for missing values\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object to Int type\n",
    "\n",
    "The `age` values in this dataset are formatted as float types (example: `19.0`) even though they are strings. Float types are typically treated as continuous quantitative values, but we may want to use `age` as a discrete quantitative data type, as we did during Modules 2 and 3. After converting the column to a numeric type (it will become a float) and replacing the string values with null, we will additionally use the `.astype()` function to truncate the decimal values and make the numbers as integers.\n",
    "\n",
    "The `.astype()` function can be used to convert columns into int, float, or object data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to numeric and replace string values w/ null\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column to an int type\n",
    "# assign changes back into the 'age' column\n",
    "df['age'] = df['age'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns\n",
    "\n",
    "It is good practice to rename columns when column names in a dataset are too long, non-descriptive, or have hidden whitespaces (causing difficulties when selecting it).\n",
    "\n",
    "The `.rename()` function uses a dictionary in the `columns=` argument, where the dictionary's key is the current column name, and the value is the new column name (similar to `.map`). Rather than directly assigning the changes back to the dataframe's variable name (in the style `df = df.whatever_function_made_changes()`), the `inplace=True` argument in the function will overwrite the dataframe's current information with the version that has the changes. This argument makes it easier to reuse the same variable name for the dataframe.\n",
    "\n",
    "***Caution***: Be careful using `inplace=True`! Because it is a permanent change, if the changes are incorrect or unexpected, you will have to run the code cells starting from the beginning. Make a copy of the dataframe and experiment with the change first, and if everything is okay, then update the original dataframe using `inplace=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the 'fname' and 'lname' columns\n",
    "df.rename(columns={'fname':'first_name', 'lname':'last_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the new column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns\n",
    "\n",
    "In Module 2, you saw an example of creating a dataframe by selecting columns to keep. Another method of creating a dataframe without columns that are not needed (this is common when preparing the dataset for predictive modeling) is to drop the columns. \n",
    "\n",
    "The `.drop()` function takes in the column(s) name(s) and then argument `axis=1` directs it to find a column with that name(s) to drop. (***Note:*** A good method of remembering `axis=1` is that `1` is vertical, just like a column.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Column\n",
    "\n",
    "When dropping a single column, give the column name directly to the `.drop()` function as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'address' column (permanent change using inplace=True)\n",
    "df.drop('address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Columns\n",
    "\n",
    "When dropping multiple columns, give a list of column names to the `.drop()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview of changes (not saved)\n",
    "df.drop(['first_name', 'last_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicate Data\n",
    "\n",
    "In Module 2, we identified rows of duplicate data using the `.duplicated()` function. If the duplicate rows are not necessary for the analysis, the `.drop_duplicates()` function will remove them from the dataset.\n",
    "\n",
    "Similar to the `.duplicated()` function, the argument `subset=` can drop duplicates for a specific column(s). By default, the `.drop_duplicates()` function keeps the first instance of the duplicate (the first time that row of data was \"seen\") and drops any other duplicate rows found afterwards. Howver, the argument `keep='last'` will keep the last instance (last \"seen\") of the duplicated row, and remove all others beforehand (including the first instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with duplicates\n",
    "names = ['Jessica','John','Bob','Jessica','Mary','John','Mel','Mel']\n",
    "grades = [95,78,76,95,77,78,99,100]\n",
    "GradeList = list(zip(names,grades))\n",
    "\n",
    "df_dupe = pd.DataFrame(data = GradeList, columns=['Names', 'Grades'])\n",
    "df_dupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows and assign changes to new dataframe\n",
    "df_nodupe = df_dupe.drop_duplicates()\n",
    "df_nodupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupe.loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupe.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupe.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'Names' duplicates only and keep the last duplicate row\n",
    "df_dupe.drop_duplicates(subset=['Names'], keep='last', inplace=True)\n",
    "df_dupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Earlier in this lesson, we identified string values that were placeholders for missing information and then replaced them with actual null values. However, missing data does not add insight to our analysis and for predictive models, it cannot use rows with missing data. Missing data is handled in two ways - 1) fill in the missing information with a \"guesstimate\" or 2) exclude/eliminate rows with missing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Missing Values\n",
    "\n",
    "Filling in a missing value with a \"estimation\" (called **imputation**) requires understanding about the dataset. Common values used to fill in missing data are averages (mean or median) or a zero value. Domain knowledge is useful when choosing an estimator value.\n",
    "\n",
    "The `.fillna()` function replaces all missing values in a column with the estimator value given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average age (of all non-null values)\n",
    "# convert it to an \"int\" type (b/c of the column's data type)\n",
    "age_mean = df['age'].mean()\n",
    "age_mean = int(age_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing ages with the mean age\n",
    "df['age'].fillna(age_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that age does not have missing values\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values\n",
    "\n",
    "When rows that have missing data are excluded/removed, it reduces the total number of rows in the dataset. \n",
    "\n",
    "***Caution:*** Some columns may have purposefully missing information, and removing those rows could result in a drastically smaller dataframe or an empty dataframe altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of rows in dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude missing values\n",
    "\n",
    "The best practice method of \"removing\" missing values is to exclude them, or in other words, select the rows that **do not** have missing information. Simialar to the `.isnull()` function, `.notnull()` assigns `True`/`False` boolean values depending on if the value is **not missing** (example: `True` that it is not missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select values that are not missing\n",
    "# assign changes to new variable\n",
    "df_nomissing = df.loc[df['gender'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in new dataframe\n",
    "len(df_nomissing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop missing values\n",
    "\n",
    "`.dropna` will remove rows from the dataframe on the condition that at least one value in the row is missing. To specify conditions for certain column(s) to check for missing values in order to drop the row, include the `subset=` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have missing values in the 'exercise' or 'hours' columns\n",
    "df.dropna(subset=['exercise', 'hours'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in the dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers\n",
    "\n",
    "Outliers are values in the dataset that are uncommon or do not \"behave\" like other values. Outliers can skew our analysis or predictive model, if we are using the data for \"normal\" or typical patterns and trends.\n",
    "\n",
    "Thr two methods we will use to exlcude outliers are the **Standard Deviation Method** and the **Interquartile Range Method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation method\n",
    "\n",
    "The Standard Deviation Method relies on the mean and standard deviation (range between mean and ~34% of the data) to determine the boundaries for outliers. Depending on the data, either 2 or 3 standard deviations is typically used for the boundary threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation for 'grades' column\n",
    "mean_grade = df['grade'].mean()\n",
    "std_grade = df['grade'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary value at +3std (3 standard deviations above the mean) and -3std (3 standard deviations below the mean)\n",
    "\n",
    "upper_std = mean_grade + (3 * std_grade)\n",
    "lower_std = mean_grade - (3 * std_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(upper_std, lower_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the rows between -3std and +3std (will filter out rows outside of those bounds)\n",
    "df_std = df.loc[(df['grade'] >= lower_std) & (df['grade'] <= upper_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare the number of rows in the dataframe before and after removing outliers\n",
    "print(len(df))\n",
    "print(len(df_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR method\n",
    "\n",
    "The Interquartile Range Method relies on the median and IQR (interquartile range - innermost 50% of data) to determine the boundaries for outliers. The boundary values are **always** 1.5 times the IQR, above and below the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the boundaries for 25th and 75th percentiles\n",
    "Q1 = df['grade'].quantile(0.25)\n",
    "Q3 = df['grade'].quantile(0.75)\n",
    "\n",
    "# calculate IQR\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate upper and lower boundaries for outliers\n",
    "upper_fence = Q3 + (1.5 * IQR)\n",
    "lower_fence = Q1 - (1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(upper_fence, lower_fence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where the grade is between the lower and upper boundaries (not an outlier)\n",
    "df_iqr = df.loc[(df['grade'] >= lower_fence) & (df['grade'] <= upper_fence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the number of rows in the dataframe before and after removing outliers\n",
    "print(len(df))\n",
    "print(len(df_iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Ordinal Categories to Discrete Numbers\n",
    "\n",
    "Most machine learning algorithms cannot use string values for the columns used to predict an outcome. For qualitative categories that have order or rank, it is good practice to assign numerical order values to each category.\n",
    "\n",
    "To reassign string values to numbers, we will use the `.map()` function. `.map` uses a dictionary, where the current value (value to reassign) is the key and the null value that it will be replaced with is the dictionary's value.\n",
    "\n",
    "***Note:*** In a later lesson module, we will discuss **one-hot encoding**, which converts nominal (non-ordered) data into a boolean value representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data\n",
    "names = ['Bob','Jessica','Mary','John','Mel']\n",
    "degrees = ['associates', 'phd', 'bachelors', 'associates', 'masters']\n",
    "degree_list = list(zip(names, degrees))\n",
    "\n",
    "df_degree = pd.DataFrame(data=degree_list, columns=['name', 'degree'])\n",
    "df_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with current values and numerical reassignment values\n",
    "degree_val = {'associates':1, 'bachelors':2, 'masters':3, 'phd':4}\n",
    "\n",
    "# give the dictionary to the .map function\n",
    "# convert degree string categories to discrete numbers\n",
    "df_degree['degree'] = df_degree['degree'].map(degree_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version of 'degree' column\n",
    "df_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Columns\n",
    "\n",
    "We sometimes need to create new columns in the dataframe, that have information from a single column (transforming it in a different way) or is a combination of two or more columns' values. There are many methods to make new columns in a dataframe, and will vary depending on what kind of transformation/manipulation you need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Math\n",
    "\n",
    "In the `pandas` library, singular columns in a dataframe behave like an array. Using calculation methods from linrar algebra, arrays of the same shape can be combined together mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample dataset\n",
    "names = ['Bob','Jessica','Mary','John','Mel']\n",
    "bs = [1,1,0,0,1]\n",
    "ms = [2,1,0,0,0]\n",
    "phd = [0,1,0,0,0]\n",
    "\n",
    "degrees_list = list(zip(names, bs, ms, phd))\n",
    "column_names = ['name', 'bachelors', 'masters', 'phd']\n",
    "\n",
    "df_degree = pd.DataFrame(data=degrees_list, columns=column_names)\n",
    "df_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'bachelors', 'masters', and 'phd' columns for total number of degrees\n",
    "df_degree['num_degrees'] = df_degree['bachelors'] + df_degree['masters'] + df_degree['phd']\n",
    "\n",
    "df_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent continuous data as discrete (binning)\n",
    "\n",
    "Continuous numerical data is good for calculating statistical values, such as averages (mean, median), standard deviations, minimum/maximum, etc. but are difficult to use when you want to analyze segments of similar groups within a range (such as people that have \"middle income\" within a particular salary range). We can create range groups from continuous data (called **binning**) to make discrete categories, represented with numbers or strings.\n",
    "\n",
    "The `.apply()` function will operate a defined function on each value of a specified column in a dataframe. (***Note:*** If you find yourself wanting to loop through each row of a dataframe to do a set of tasks to a column, you most likely need to use the `.apply()` function. Looping on a dataframe is difficult and time/resource consuming.)\n",
    "\n",
    "- use .apply (using def function on one column)\n",
    "- lettergrade example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that checks the grade value and assigns it a letter grade category\n",
    "def lettergrade(grade_num):\n",
    "    if grade_num < 60: # grade is 0-59\n",
    "        grade ='F'\n",
    "    elif grade_num < 70: # grade is 60-69\n",
    "        grade = 'D'\n",
    "    elif grade_num < 80: # grade is 70-79\n",
    "        grade = 'C'\n",
    "    elif grade_num < 90: # grade is 80-89\n",
    "        grade = 'B'\n",
    "    else: # grade is 90+\n",
    "        grade = 'A'\n",
    "        \n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the 'lettergrade' function using the 'grade' column as the data source\n",
    "# create a new column to store the new letter grade values\n",
    "df['lettergrade'] = df['grade'].apply(lettergrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of students per letter grade\n",
    "pd.value_counts(df['lettergrade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average hours of study per letter grade\n",
    "df.groupby('lettergrade')['hours'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multiple columns\n",
    "\n",
    "If you are using more than simple math to combine two or more columns' information, then you can define a function with the set of tasks and pass the columns into the function using the syntax `df[new_col] = func(col1, col2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will combine two strings with a whitespace between them\n",
    "# create a fullname for the student\n",
    "def fullname(fname, lname):\n",
    "    full_name = fname + \" \" + lname\n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'first_name' and 'last_name' columns in the function\n",
    "df['fullname'] = fullname(df['first_name'], df['last_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make column headers into categories in a single column\n",
    "\n",
    "Dataset structure is important for the kind of tasks that we want to do, not only for analysis in Python but also for data visualization. Often, categorical data within a single characteristic have their own separate columns and it makes it difficult to compare them, particularly for charts such as pie, bar, etc. \n",
    "\n",
    "The `pd.melt()` function takes columns that are categories, and reduces them to a single column. `id_vars=` takes a list of columns that **should not** be made into categories (are not included in the melt). `var_name=` assigns the new name of the column with categories, and `value_name` is the name of the columnthat stores the values that used to be in the categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample dataset\n",
    "regions = ['East', 'South', 'Central', 'West']\n",
    "q1_rev = [1500, 1200, 900, 8000]\n",
    "q2_rev = [3000, 700, 1300, 6500]\n",
    "q3_rev = [2700, 1000, 2600, 9400]\n",
    "q4_rev = [4300, 1800, 3500, 9400]\n",
    "quarter_rev = list(zip(regions, q1_rev, q2_rev, q3_rev, q4_rev))\n",
    "\n",
    "column_names = ['region', 'Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "df_rev = pd.DataFrame(data=quarter_rev, columns=column_names)\n",
    "df_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart can only view one quarter at a time\n",
    "# cannot compares all quarters again each other\n",
    "sns.barplot(data=df_rev, x='region', y='Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_new_rows = num_old_rows * num_columns_melted\n",
    "\n",
    "# column to NOT create into categories\n",
    "do_not_melt = ['region']\n",
    "\n",
    "# make a new column called 'quarter' to hold categories and 'revenue' will hold their values\n",
    "melt_df = pd.melt(df_rev, id_vars=do_not_melt, var_name='quarter', value_name='revenue')\n",
    "melt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can compare revenue for all the quarters across regions\n",
    "sns.barplot(data=melt_df, x='quarter', y='revenue', hue='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
